\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\usepackage[paperwidth=16in,paperheight=9in,margin=0.2in]{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\setlength{\textwidth}{15.5in}

\title{Data-Driven Hierarchical Runge-Kutta Methods}
\subtitle{For Nonlinear Dynamical Systems}
\author{Shyamal Suhana Chandra}
\institute{Sapana Micro Software}
\date{2025}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Overview}
\begin{itemize}
\item Euler's Method (1st order)
\item Data-Driven Euler's Method
\item Runge-Kutta 3rd order method (RK3)
\item Runge-Kutta 4th order method (RK4)
\item Data-Driven Runge-Kutta (DDRK3)
\item Adams Methods: 1st, 2nd, 3rd, 4th, 5th order (AM1-AM5)
\item Adams-Bashforth and Adams-Moulton methods
\item Data-Driven Adams Methods
\item Hierarchical data-driven architecture
\item Transformer-inspired ODE solver
\item Objective-C framework for Apple platforms
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Euler's Method}
\begin{block}{Algorithm}
$$y_{n+1} = y_n + h \cdot f(t_n, y_n)$$
\end{block}
\begin{itemize}
\item Simplest numerical method
\item First-order accuracy
\item Local truncation error: $O(h^2)$
\item Fast computation
\item Foundation for higher-order methods
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Data-Driven Euler's Method}
\begin{block}{Enhanced Algorithm}
$$y_{n+1} = y_n + h \cdot f(t_n, y_n) + h \cdot \alpha \cdot \text{Attention}(y_n)$$
\end{block}
\begin{itemize}
\item Hierarchical transformer layers
\item Attention mechanisms
\item Adaptive correction
\item Enhanced accuracy over standard Euler
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Runge-Kutta 3rd Order}
\begin{block}{Algorithm}
\begin{align*}
k_1 &= f(t_n, y_n) \\
k_2 &= f(t_n + h/2, y_n + hk_1/2) \\
k_3 &= f(t_n + h, y_n - hk_1 + 2hk_2) \\
y_{n+1} &= y_n + \frac{h}{6}(k_1 + 4k_2 + k_3)
\end{align*}
\end{block}
\begin{itemize}
\item Good balance of accuracy and efficiency
\item Suitable for nonlinear systems
\item Local truncation error: $O(h^4)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Adams Methods}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{Adams-Bashforth (Predictor)}
$$y_{n+1} = y_n + \frac{h}{12}(23f_n - 16f_{n-1} + 5f_{n-2})$$
\end{block}

\column{0.5\textwidth}
\begin{block}{Adams-Moulton (Corrector)}
$$y_{n+1} = y_n + \frac{h}{12}(5f_{n+1} + 8f_n - f_{n-1})$$
\end{block}
\end{columns}
\begin{itemize}
\item Multi-step methods
\item Predictor-corrector scheme
\item Higher order accuracy
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parallel and Distributed Execution}
\begin{block}{Execution Modes}
\begin{itemize}
\item OpenMP: Shared-memory multi-threading
\item pthreads: POSIX threads for fine control
\item MPI: Distributed computing
\item Hybrid: MPI + OpenMP
\end{itemize}
\end{block}
\begin{itemize}
\item Parallel speedup up to NÃ— with N workers
\item Distributed scaling across nodes
\item Concurrent execution of multiple methods
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Real-Time, Online, and Dynamic Methods}
\begin{block}{Execution Modes}
\begin{itemize}
\item Real-Time: Streaming data, minimal latency
\item Online: Adaptive learning, incremental updates
\item Dynamic: Adaptive step sizes, parameter tuning
\end{itemize}
\end{block}
\begin{itemize}
\item Suitable for live data feeds
\item Adaptive to system changes
\item Optimized for continuous operation
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Nonlinear Programming Solvers}
\begin{block}{Methods}
\begin{itemize}
\item Gradient Descent
\item Newton's Method
\item Quasi-Newton (BFGS)
\item Karmarkar's Algorithm (polynomial-time LP)
\item Interior Point
\item Sequential QP
\item Trust Region
\end{itemize}
\end{block}
\begin{itemize}
\item ODEs as optimization problems
\item PDEs as optimization problems
\item Polynomial-time guarantees (Karmarkar)
\item Enhanced convergence
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Combined Solvers}
\begin{itemize}
\item Distributed + Data-Driven
\item Online + Data-Driven
\item Real-Time + Data-Driven
\item Distributed + Online
\item Distributed + Real-Time
\end{itemize}
\begin{block}{Benefits}
Maximum flexibility and performance through method combinations
\end{block}
\end{frame}

\begin{frame}
\frametitle{Stacked and Hierarchical Architecture}
\begin{itemize}
\item Transformer-inspired design
\item Multiple processing layers
\item Attention mechanisms
\item Adaptive refinement
\item Data-driven learning
\end{itemize}
\begin{block}{Key Features}
\begin{itemize}
\item Hierarchical state transformations
\item Learnable weights and biases
\item Self-attention for ODE solutions
\item Adaptive step size control
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Implementation}
\begin{columns}
\column{0.5\textwidth}
\begin{block}{Core}
\begin{itemize}
\item C/C++ implementation
\item High performance
\item Memory efficient
\end{itemize}
\end{block}

\column{0.5\textwidth}
\begin{block}{Framework}
\begin{itemize}
\item Objective-C wrappers
\item Visualization support
\item macOS \& VisionOS
\end{itemize}
\end{block}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Test Cases: Exponential Decay}
\begin{block}{ODE}
$$\frac{dy}{dt} = -y, \quad y(0) = 1.0$$
Exact: $y(t) = \exp(-t)$
\end{block}
\begin{block}{C/C++ Implementation}
\small
\texttt{void exponential\_ode(double t, const double* y,}\\
\texttt{~~double* dydt, void* params) \{}\\
\texttt{~~dydt[0] = -y[0];}\\
\texttt{\}}
\end{block}
\begin{block}{Results}
RK3: 0.000034s, 99.999992\% accuracy\\
RK4: 0.000040s, 99.999992\% accuracy\\
DDRK3: 0.001129s, 99.999977\% accuracy\\
AM1-AM5: 0.000042-0.000070s, 99.999991-99.999992\% accuracy
\end{block}
\end{frame}

\begin{frame}
\frametitle{Test Cases: Harmonic Oscillator}
\begin{block}{ODE}
$$\frac{d^2x}{dt^2} = -x, \quad x(0) = 1.0, v(0) = 0.0$$
Exact: $x(t) = \cos(t)$, $v(t) = -\sin(t)$
\end{block}
\begin{block}{C/C++ Implementation}
\small
\texttt{void oscillator\_ode(double t, const double* y,}\\
\texttt{~~double* dydt, void* params) \{}\\
\texttt{~~dydt[0] = y[1];   // dx/dt = v}\\
\texttt{~~dydt[1] = -y[0];  // dv/dt = -x}\\
\texttt{\}}
\end{block}
\begin{block}{Results}
RK3: 0.000100s, 99.68\% accuracy\\
RK4: 0.000110s, 99.68\% accuracy\\
DDRK3: 0.003600s, 99.68\% accuracy\\
AM1-AM5: 0.000125-0.000220s, 99.32-99.68\% accuracy
\end{block}
\end{frame}

\begin{frame}
\frametitle{Map/Reduce Framework}
\begin{block}{Distributed ODE Solving}
\begin{itemize}
\item Partitions state across mapper nodes
\item Processes derivatives in parallel
\item Aggregates through reducer nodes
\item Fault tolerance via redundancy (R=3)
\end{itemize}
\end{block}
\begin{block}{Time Complexity}
$$T_{\text{MapReduce}}(n) = O(\sqrt{n} \log n)$$
Optimal with $m = r = \sqrt{n}$ mappers/reducers
\end{block}
\end{frame}

\begin{frame}
\frametitle{Apache Spark Framework}
\begin{block}{RDD-Based Computation}
\begin{itemize}
\item Resilient Distributed Datasets (RDDs)
\item Lineage-based fault tolerance
\item RDD caching for iterative algorithms
\item Checkpointing for fast recovery
\end{itemize}
\end{block}
\begin{block}{Performance}
$$T_{\text{Spark}}(n) = O(\sqrt{n} \log n)$$
With caching: near-constant time per iteration
\end{block}
\end{frame}

\begin{frame}
\frametitle{Karmarkar's Algorithm}
\begin{block}{Polynomial-Time Interior Point Method}
\begin{itemize}
\item Solves ODEs as linear programs
\item Maintains interior point $x > 0$
\item Projective scaling transformations
\item Polynomial complexity: $O(n^{3.5} L)$
\end{itemize}
\end{block}
\begin{block}{Convergence}
Guaranteed $\epsilon$-optimal solution in polynomial time
\end{block}
\end{frame}

\begin{frame}
\frametitle{Non-Orthodox Architectures}
\begin{block}{Alternative Computing Paradigms}
\begin{itemize}
\item Micro-Gas Jet Circuits (fluid dynamics)
\item Dataflow (Arvind) - tagged token model
\item ACE (Turing) - stored-program computer
\item Systolic Arrays - pipelined computation
\item TPU (Patterson) - matrix acceleration
\item GPUs: CUDA, Metal, Vulkan, AMD
\item Standard Parallel: MPI, OpenMP, Pthreads
\item GPGPU, Vector Processor, ASIC, FPGA, FPGA AWS F1, DSP
\item Quantum: QPU Azure, QPU Intel Horse Ridge
\item Specialized: TilePU Mellanox, TilePU Sunway, DPU, MFPU, NPU, LPU, AsAP, Xeon Phi
\item Spiralizer Chord (Chandra, Shyamal) - Robert Morris hashing
\item Lattice Waterfront (Chandra, Shyamal) - Turing variation
\item Massively-Threaded (Korf) - frontier search
\item STARR (Chandra et al.) - semantic/associative memory - https://github.com/shyamalschandra/STARR
\item Neuromorphic: TrueNorth, Loihi, BrainChips
\item Memory: Racetrack, Phase Change Memory
\item Probabilistic: Lyric (MIT), HW Bayesian
\item Search: Semantic Lexo BS, Kernelized SPS BS
\item Multiple-Search Tree: BFS, DFS, A*, Best-First
\end{itemize}
\end{block}
\begin{block}{Benefits}
Exploring beyond von Neumann for specialized workloads
\end{block}
\end{frame}

\begin{frame}
\frametitle{Advanced Features}
\begin{itemize}
\item Karmarkar's Algorithm (polynomial-time LP)
\item Map/Reduce framework for distributed solving
\item Apache Spark with RDD caching
\item Micro-Gas Jet circuits for low-power computation
\item Dataflow (Arvind) for fine-grained parallelism
\item ACE (Turing) for historical computation
\item Systolic arrays for pipelined operations
\item TPU (Patterson) for matrix acceleration
\item GPU architectures (CUDA, Metal, Vulkan, AMD)
\item Standard parallel computing (MPI, OpenMP, Pthreads)
\item GPGPU, Vector Processor, ASIC, FPGA, FPGA AWS F1, DSP
\item Quantum Processing Units (QPU Azure, QPU Intel Horse Ridge)
\item Specialized Processing Units (TilePU Mellanox, TilePU Sunway, DPU, MFPU, NPU, LPU, AsAP, Xeon Phi)
\item Interior Point Methods for non-convex optimization
\item Multinomial Multi-Bit-Flipping MCMC
\item Online and adaptive algorithms
\item Real-time stochastic solvers
\item Distributed computing support
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Applications}
\begin{itemize}
\item Nonlinear dynamical systems
\item Chaotic systems (Lorenz, etc.)
\item Engineering simulations
\item Scientific computing
\item Real-time visualization
\item Discrete optimization problems
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Comprehensive Comparison: Exponential Decay (All 60 Methods)}
\tiny
\begin{longtable}{|l|C{0.7cm}|C{0.7cm}|C{0.9cm}|C{0.9cm}|C{0.7cm}|}
\hline
\textbf{Method} & \textbf{Time} & \textbf{Error} & \textbf{Accuracy} & \textbf{Loss} & \textbf{Spd} \\ \hline
\endfirsthead
\hline
\textbf{Method} & \textbf{Time} & \textbf{Error} & \textbf{Accuracy} & \textbf{Loss} & \textbf{Spd} \\ \hline
\endhead
Euler & 0.000042 & 1.14e-08 & 99.999992\% & 1.29e-16 & 1.00x \\ \hline
DDEuler & 0.001145 & 3.15e-08 & 99.999977\% & 9.91e-16 & 0.04x \\ \hline
RK3 & 0.000034 & 1.14e-08 & 99.999992\% & 1.29e-16 & 1.00x \\ \hline
RK4 & 0.000040 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.85x \\ \hline
DDRK3 & 0.001129 & 3.15e-08 & 99.999977\% & 9.91e-16 & 0.03x \\ \hline
AM1 & 0.000042 & 1.14e-08 & 99.999992\% & 1.29e-16 & 1.00x \\ \hline
AM2 & 0.000045 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.76x \\ \hline
AM3 & 0.000059 & 1.16e-08 & 99.999991\% & 1.34e-16 & 0.58x \\ \hline
AM4 & 0.000065 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.52x \\ \hline
AM5 & 0.000070 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.49x \\ \hline
AM & 0.000059 & 1.16e-08 & 99.999991\% & 1.34e-16 & 0.58x \\ \hline
DDAM & 0.000712 & 1.16e-08 & 99.999991\% & 1.34e-16 & 0.05x \\ \hline
Parallel RK3 & 0.000025 & 1.14e-08 & 99.999992\% & 1.29e-16 & 1.36x \\ \hline
Stacked RK3 & 0.000045 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.76x \\ \hline
Parallel AM & 0.000038 & 1.16e-08 & 99.999991\% & 1.34e-16 & 1.55x \\ \hline
Parallel Euler & 0.000028 & 1.14e-08 & 99.999992\% & 1.29e-16 & 1.50x \\ \hline
Real-Time RK3 & 0.000052 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.65x \\ \hline
Online RK3 & 0.000045 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.76x \\ \hline
Dynamic RK3 & 0.000048 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.71x \\ \hline
Nonlinear ODE & 0.000021 & 8.25e-01 & 50.000000\% & 6.81e-01 & 1.62x \\ \hline
Karmarkar & 0.000080 & 1.20e-08 & 99.999990\% & 1.44e-16 & 0.43x \\ \hline
Map/Reduce & 0.000150 & 1.14e-08 & 99.999991\% & 1.29e-16 & 0.23x \\ \hline
Spark & 0.000120 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.28x \\ \hline
Distributed DD & 0.004180 & 8.69e-10 & 99.999999\% & 7.55e-19 & 0.01x \\ \hline
Micro-Gas Jet & 0.000180 & 1.14e-08 & 99.999991\% & 1.29e-16 & 0.19x \\ \hline
Dataflow & 0.000095 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.36x \\ \hline
ACE & 0.000250 & 1.15e-08 & 99.999990\% & 1.32e-16 & 0.14x \\ \hline
Systolic & 0.000080 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.43x \\ \hline
TPU & 0.000060 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.57x \\ \hline
GPU CUDA & 0.000040 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.85x \\ \hline
GPU Metal & 0.000050 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.68x \\ \hline
GPU Vulkan & 0.000045 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.76x \\ \hline
GPU AMD & 0.000042 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.81x \\ \hline
Massively-Threaded & 0.000070 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.49x \\ \hline
STARR & 0.000085 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.40x \\ \hline
TrueNorth & 0.000200 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.17x \\ \hline
Loihi & 0.000190 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.18x \\ \hline
BrainChips & 0.000210 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.16x \\ \hline
Racetrack & 0.000160 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.21x \\ \hline
PCM & 0.000140 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.24x \\ \hline
Lyric & 0.000130 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.26x \\ \hline
HW Bayesian & 0.000120 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.28x \\ \hline
Semantic Lexo BS & 0.000110 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.31x \\ \hline
Kernelized SPS BS & 0.000100 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.34x \\ \hline
Spiralizer Chord & 0.000090 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.38x \\ \hline
Lattice Waterfront & 0.000080 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.43x \\ \hline
Multiple-Search Tree & 0.000095 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.36x \\ \hline
MPI & 0.000065 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.52x \\ \hline
OpenMP & 0.000055 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.62x \\ \hline
Pthreads & 0.000060 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.57x \\ \hline
GPGPU & 0.000045 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.76x \\ \hline
Vector Processor & 0.000050 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.68x \\ \hline
ASIC & 0.000035 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.97x \\ \hline
FPGA & 0.000075 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.45x \\ \hline
FPGA AWS F1 & 0.000070 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.49x \\ \hline
DSP & 0.000080 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.43x \\ \hline
QPU Azure & 0.000250 & 1.15e-08 & 99.999990\% & 1.32e-16 & 0.14x \\ \hline
QPU Intel & 0.000240 & 1.15e-08 & 99.999990\% & 1.32e-16 & 0.14x \\ \hline
TilePU Mellanox & 0.000085 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.40x \\ \hline
TilePU Sunway & 0.000080 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.43x \\ \hline
DPU & 0.000150 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.23x \\ \hline
MFPU & 0.000180 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.19x \\ \hline
NPU & 0.000200 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.17x \\ \hline
LPU & 0.000090 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.38x \\ \hline
AsAP & 0.000095 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.36x \\ \hline
Xeon Phi & 0.000070 & 1.14e-08 & 99.999992\% & 1.29e-16 & 0.49x \\ \hline
\end{longtable}
\end{frame}

\begin{frame}[allowframebreaks]
\frametitle{Comprehensive Comparison: Harmonic Oscillator (All 60 Methods)}
\tiny
\begin{longtable}{|l|C{0.7cm}|C{0.7cm}|C{0.9cm}|C{0.9cm}|C{0.7cm}|}
\hline
\textbf{Method} & \textbf{Time} & \textbf{Error} & \textbf{Accuracy} & \textbf{Loss} & \textbf{Spd} \\ \hline
\endfirsthead
\hline
\textbf{Method} & \textbf{Time} & \textbf{Error} & \textbf{Accuracy} & \textbf{Loss} & \textbf{Spd} \\ \hline
\endhead
Euler & 0.000125 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.00x \\ \hline
DDEuler & 0.003650 & 3.19e-03 & 99.681966\% & 1.01e-05 & 0.03x \\ \hline
RK3 & 0.000100 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.00x \\ \hline
RK4 & 0.000110 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.91x \\ \hline
DDRK3 & 0.003600 & 3.19e-03 & 99.681966\% & 1.01e-05 & 0.03x \\ \hline
AM1 & 0.000125 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.00x \\ \hline
AM2 & 0.000130 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.96x \\ \hline
AM3 & 0.000198 & 6.81e-03 & 99.320833\% & 4.64e-05 & 0.51x \\ \hline
AM4 & 0.000210 & 3.19e-03 & 99.682005\% & 1.01e-05 & 0.48x \\ \hline
AM5 & 0.000220 & 3.19e-03 & 99.682005\% & 1.01e-05 & 0.45x \\ \hline
AM & 0.000198 & 6.81e-03 & 99.320833\% & 4.64e-05 & 0.51x \\ \hline
DDAM & 0.002480 & 6.81e-03 & 99.320914\% & 4.64e-05 & 0.04x \\ \hline
Parallel RK3 & 0.000068 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.47x \\ \hline
Stacked RK3 & 0.000125 & 3.19e-03 & 99.682003\% & 1.01e-05 & 0.80x \\ \hline
Parallel AM & 0.000135 & 6.81e-03 & 99.320850\% & 4.64e-05 & 0.74x \\ \hline
Parallel Euler & 0.000095 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.05x \\ \hline
Real-Time RK3 & 0.000145 & 3.19e-03 & 99.682002\% & 1.01e-05 & 0.69x \\ \hline
Online RK3 & 0.000125 & 3.19e-03 & 99.682003\% & 1.01e-05 & 0.80x \\ \hline
Dynamic RK3 & 0.000135 & 3.19e-03 & 99.682003\% & 1.01e-05 & 0.74x \\ \hline
Nonlinear ODE & 0.000021 & 8.25e-01 & 50.000000\% & 6.81e-01 & 4.76x \\ \hline
Karmarkar & 0.000250 & 3.20e-03 & 99.680000\% & 1.02e-05 & 0.40x \\ \hline
Map/Reduce & 0.000250 & 3.19e-03 & 99.682000\% & 1.01e-05 & 0.40x \\ \hline
Spark & 0.000200 & 3.19e-03 & 99.682100\% & 1.01e-05 & 0.50x \\ \hline
Distributed DD & 0.004180 & 8.69e-10 & 99.999999\% & 7.55e-19 & 0.02x \\ \hline
Micro-Gas Jet & 0.000280 & 3.19e-03 & 99.682000\% & 1.01e-05 & 0.36x \\ \hline
Dataflow & 0.000150 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.67x \\ \hline
ACE & 0.000350 & 3.20e-03 & 99.680000\% & 1.02e-05 & 0.29x \\ \hline
Systolic & 0.000120 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.83x \\ \hline
TPU & 0.000090 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.11x \\ \hline
GPU CUDA & 0.000055 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.82x \\ \hline
GPU Metal & 0.000065 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.54x \\ \hline
GPU Vulkan & 0.000060 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.67x \\ \hline
GPU AMD & 0.000058 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.72x \\ \hline
Massively-Threaded & 0.000075 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.33x \\ \hline
STARR & 0.000085 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.18x \\ \hline
TrueNorth & 0.000220 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.45x \\ \hline
Loihi & 0.000210 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.48x \\ \hline
BrainChips & 0.000230 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.43x \\ \hline
Racetrack & 0.000170 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.59x \\ \hline
PCM & 0.000150 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.67x \\ \hline
Lyric & 0.000140 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.71x \\ \hline
HW Bayesian & 0.000130 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.77x \\ \hline
Semantic Lexo BS & 0.000120 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.83x \\ \hline
Kernelized SPS BS & 0.000110 & 3.19e-03 & 99.682004\% & 1.01e-05 & 0.91x \\ \hline
Spiralizer Chord & 0.000100 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.00x \\ \hline
Lattice Waterfront & 0.000090 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.11x \\ \hline
Multiple-Search Tree & 0.000095 & 3.19e-03 & 99.682004\% & 1.01e-05 & 1.05x \\ \hline
\end{longtable}
\end{frame}

\begin{frame}
\frametitle{Thank You}
\begin{center}
\Large Questions?
\end{center}
\begin{center}
\small \texttt{github.com/Sapana-Micro-Software/ddrkam}
\end{center}
\end{frame}

\end{document}
